template <int DT, int TIMETOL, int BATCH>
void perform_requests(int NNodes,
                      ReferenceContainer REF,
                      unsigned long N,
                      OpenMPHelper &O) {

    std::uniform_int_distribution<int> gen(92412,732532);
    unsigned int SEED = std::stoi(std::getenv("SEED"));
    std::mt19937 rng(SEED);
    int randi = gen(rng);


    long ix;
    int total_processed=0;
    int atomic_helper;

    bool globalstatus = true;
    bool ix_update = false;
    long current_ix;

    bool all_sent_locally = false;
    int tot_locals = 0;
    int sent_locally = 0;

    // Insert here required initialization
    std::list<long> all_indexes_seen;
    if (BATCH<=0)error_report(min_batch_msg);
    bool waiting = false;
    bool bypass = false;
    bool was_last_appearance = false;
    int special_index = -1;
    int rstatus = 0, sstatus = 0;
    MPI_Request requests_send[BATCH];
    int fsend[BATCH] = {0};
    int frecv[BATCH] = {0};
    int areRecv[BATCH] = {0};
    int myProbes[BATCH] = {0};
    MPI_Request requests_recv[BATCH];
    InfoVecElem results[BATCH];
    std::queue<int> QAvailable;
    std::list<int> QPend;
    for (int i = 0; i < BATCH; ++i){
        QAvailable.push(i);
    }
    double vval[BATCH], their_vix[BATCH];
    int ixlist[BATCH]; // this is our new helper in order to process various indexes asynchronously (=^-|)-|-/
    int owner[BATCH];
    int retStatus=1;
    int status_rstatus=1;
    int counter=0,MAX_TRIES=25; // #HYPERPARAMS #HYPERPARAMETERS
    bool NONBLOCKING = true;

#pragma omp atomic read
    atomic_helper = *(REF.p_TOT);
    globalstatus = (atomic_helper < NNodes);

    while (globalstatus) {
        // Perform the main task: ask for the required information!
        if (ix_update) {
            auto itBeg = REF.p_ParHelper->data[ix].MissingA.begin();
            auto itEnd = REF.p_ParHelper->data[ix].MissingA.end();
            tot_locals  = 0;
            sent_locally = 0;
            total_processed = 0;
            int localcounter = 0;

            for (auto _it= itBeg; _it != itEnd; ++_it) {
                auto &thread = *_it;
                for (auto it =  thread.begin();it != thread.end(); it++) {
                     ++tot_locals;
                }
            }
            PRINTF_DBG("Tot locals for index  %d is %d\n", ix, tot_locals);

            // race-condition unfriendly? Only extensive testing will help us decide :^|
            for (auto _it= itBeg; _it != itEnd; ++_it) {
                PRINTF_DBG("About to ask for the process of local elem ix: %d\n",ix);
                auto &thread = *_it;
                PRINTF_DBG("SURVIVED 1 \n");
                for (auto it =  thread.begin();it != thread.end(); it++){
                    ++localcounter;
                    PRINTF_DBG("SURVIVED 2 \n");

                    // Retrieve data
                    owner[QAvailable.front()] = std::get<1>(*it);
                    their_vix[QAvailable.front()] = std::get<2>(*it);
                    // Build the result
                    results[QAvailable.front()] = std::make_tuple(0, // placeholder until we get the correct val
                                                      std::get<0>(*it), // we are inaugurating this indexing model [:<)
                                                      owner[QAvailable.front()] * N + their_vix[QAvailable.front()]);
                    PRINTF_DBG("SURVIVED 3 \n");


                    // Debug Station
//                    PRINTF_DBG("Sending and recieving (nonblocking): asking %d for vertex w index: %f \n",
//                           owner[QAvailable.front()], their_vix[QAvailable.front()]);
//                    std::cout  << std::flush;
//                    PRINTF_DBG("SURVIVED 4 \n");


                    // Asynchronously start a synchronized sending operation
                    if (NONBLOCKING) {
                        retStatus = 1;
                        requests_send[QAvailable.front()] = MPI_Request();
                        while (retStatus != 0) {
                            retStatus = MPI_Isend(&their_vix[QAvailable.front()],
                                                   1,
                                                   MPI_DOUBLE,
                                                   owner[QAvailable.front()],
                                                   VERTEXVAL_REQUEST_FLAG,
                                                   MPI_COMM_WORLD,
                                                   &requests_send[QAvailable.front()]);
                        }
                        fsend[QAvailable.front()] = 0;
                        MPI_Test(&requests_send[QAvailable.front()],
                                 &fsend[QAvailable.front()],
                                 MPI_STATUS_IGNORE);
                    } else {
                        PRINTF_DBG("Starting blocking ssend");
                        std::cout << std::flush;
                        MPI_Ssend(&their_vix[QAvailable.front()],
                                  1,
                                  MPI_DOUBLE,
                                  owner[QAvailable.front()],
                                  VERTEXVAL_REQUEST_FLAG,
                                  MPI_COMM_WORLD);
                        PRINTF_DBG("Ending blocking ssend");
                        std::cout << std::flush;
                        fsend[QAvailable.front()] = 1;
                    }

                    // fsend will monitor if it has arrived
                    frecv[QAvailable.front()] = 0;
                    areRecv[QAvailable.front()] = 0;
                    myProbes[QAvailable.front()] = 0;


                    // Prepare stuff  for the next iteration
                    QPend.push_back(QAvailable.front());
                    ixlist[QAvailable.front()] = ix;
                    QAvailable.pop();
                    PRINTF_DBG("SURVIVED 9 \n");

                    // If QAvailable is empty, devote ourselves to asynchronously waiting for
                    // answers to arrive. This indirectly means that our QPend has reached length BATCH.
                    if ((QAvailable.empty()) || (localcounter == tot_locals)) {

                        // Define a target  size that QPend is required to meet.
                        // a progressive emptying could be coded here :^)
                        int target_size = 0;
                        if (localcounter == tot_locals){
                            // If it is the last lap for this index,
                            // force the emptying of the container :-)
                            target_size = 0;
                        } else {
                            // If it is not the last lap, just free one
                            // element.
                            target_size = BATCH-1;
                        }


                        while (QPend.size() != target_size) {

                            // Iterate through the indexes FIFO style, i.e. std::list travelling
                            auto i = QPend.begin();
                            while (i != QPend.end()) {

                                if (fsend[*i] != 1) {
                                    PRINTF_DBG("Case 1\n");std::cout<<std::flush;
                                    // If the message has not arrived
                                    // Update the value to check if it has not arrived
                                    MPI_Test(&requests_send[*i],
                                                 &fsend[*i],
                                                 MPI_STATUS_IGNORE);
                                    PRINTF_DBG("Case 1.1\n");std::cout<<std::flush;
                                    if (fsend[*i] != 1){
                                        PRINTF_DBG("Case 1.2.1\n");std::cout<<std::flush;
                                        // If it still has not arrived, we will cancel it and resend it
                                        retStatus = MPI_Cancel(&requests_send[*i]);
                                        if (retStatus!=0) PRINTF_DBG("[warning] MPI_Cancel returned %d\n",retStatus);
                                        MPI_Request_free(&requests_send[*i]);
                                        requests_send[*i] = MPI_Request();
                                        PRINTF_DBG("Case 1.2.2\n");std::cout<<std::flush;
                                        retStatus = MPI_Isend(&their_vix[*i],
                                                               1,
                                                               MPI_DOUBLE,
                                                               owner[*i],
                                                               VERTEXVAL_REQUEST_FLAG,
                                                               MPI_COMM_WORLD,
                                                               &requests_send[*i]);
                                        if (retStatus!=0) PRINTF_DBG("[warning] MPI_Isend returned %d\n",retStatus);
                                        PRINTF_DBG("Case 1.2.3\n");std::cout<<std::flush;
                                    }
                                    PRINTF_DBG("Case 1.3\n");std::cout<<std::flush;
                                }
                                if ((fsend[*i] == 1) && (frecv[*i] != 1)){
                                    PRINTF_DBG("Case 2\n");std::cout<<std::flush;
                                    // If it has now arrived, check if there is a started reception
                                    if (areRecv[*i] != 1) {
                                        PRINTF_DBG("Case 2.1.1\n");std::cout<<std::flush;
                                        //----------------------------
                                        myProbes[*i] = 1; // hardcoded
                                        //----------------------------
                                        counter = 0;
                                        while ((myProbes[*i] != 1) && (counter < MAX_TRIES)){
                                            if (counter == 0) PRINTF_DBG("Case 2.2.1 (first lap)\n");std::cout<<std::flush;
                                            retStatus = 1;
                                            while (retStatus != 0){
                                                retStatus = MPI_Iprobe(owner[*i],
                                                                       (int) their_vix[*i],
                                                                       MPI_COMM_WORLD,
                                                                       &myProbes[*i],
                                                                       MPI_STATUS_IGNORE);
                                            }
                                            counter++;
                                            //mssleep(DT);
                                        }
                                        PRINTF_DBG("Case 2.3\n");std::cout<<std::flush;
                                        if (myProbes[*i] == 1){
                                            retStatus = 1;
                                            PRINTF_DBG("Case 2.3.1\n");std::cout<<std::flush;
                                            requests_recv[*i] = MPI_Request();
                                            PRINTF_DBG("Case 2.3.2\n");std::cout<<std::flush;
                                            recv_nonblocking(owner[*i],
                                                             requests_recv[*i],
                                                             vval[*i],
                                                             (int) their_vix[*i]);
                                            areRecv[*i] = 1;
                                            PRINTF_DBG("Case 2.3.3\n");std::cout<<std::flush;
                                        }
                                        PRINTF_DBG("Case 2.1-2-3\n");std::cout<<std::flush;
                                    } else if (areRecv[*i] == 1){
                                        PRINTF_DBG("Case 2.5\n");std::cout<<std::flush;
                                        MPI_Test(&requests_recv[*i],
                                                 &frecv[*i],
                                                 MPI_STATUS_IGNORE);
                                        PRINTF_DBG("Case 2.4.1\n");std::cout<<std::flush;
                                        counter = 0;
                                        while ((frecv[*i] != 1) && (counter < MAX_TRIES)){
                                            //PRINTF_DBG("Case 2.4.2\n");std::cout<<std::flush;
                                            MPI_Test(&requests_recv[*i],
                                                     &frecv[*i],
                                                     MPI_STATUS_IGNORE);
                                            counter++;
                                            //mssleep(DT);
                                        }
                                        PRINTF_DBG("Case 2.5\n");std::cout<<std::flush;
                                        if (frecv[*i] != 1){
                                            PRINTF_DBG("Case 2.6.1\n");std::cout<<std::flush;
                                            // If it still has not arrived, we will cancel it and resend it
                                            retStatus = MPI_Cancel(&requests_recv[*i]);
                                            if (retStatus!=0) PRINTF_DBG("[warning] MPI_Cancel returned %d\n",retStatus);
                                            MPI_Request_free(&requests_recv[*i]);
                                            frecv[*i] = 0;
                                            myProbes[*i] = 0;
                                            PRINTF_DBG("Case 2.6.2\n");std::cout<<std::flush;
                                        }
                                    }
                                }

                                if ((fsend[*i] == 1) && (frecv[*i] == 1)){
                                    PRINTF_DBG("Case 3\n");std::cout<<std::flush;
                                    // If it has arrived, store it
                                    std::get<0>(results[*i]) = vval[*i];
                                    special_index = ixlist[*i];
                                    (*REF.p_IntHelper)[special_index].ResultsPendProcess.push_back(results[*i]);

                                    // Debug station
                                    PRINTF_DBG("Recieved a response to our request: val %f for ix: %d\n", vval[*i], special_index); std::cout << std::flush;

                                    // Clean
                                    if (NONBLOCKING) MPI_Request_free(&requests_send[*i]);
                                    MPI_Request_free(&requests_recv[*i]);
                                    QAvailable.push(*i);
                                    // Erasing list index
                                    QPend.erase(i++);



                                } else { // Index increase for the entire inner loop
                                        ++i;
                                }

                                // End of one iteration
                            }  // End of all iterations
                            // Now "QPend.size() != target_size" will determine if the while continues.
                            PRINTF_DBG("About to compute 'QPend.size() != target_size', which yields: %d\n", QPend.size() != target_size); std::cout<< std::flush;
                            // Optional:
                            //if (QPend.size() == BATCH) answer_messages_edges<0, 1, BATCH>(REF, O.MY_THREAD_n);
                        }
                    }
                    // Debug station
                    PRINTF_DBG("DISPATCHED CORRECTLY\n");
                    std::cout << std::flush;
                }
            }
//          *******************THE SAME SHOULD BE DONE FOR EDGES PLEASE********************
//          *******************THE SAME SHOULD BE DONE FOR EDGES PLEASE********************
//          *******************THE SAME SHOULD BE DONE FOR EDGES PLEASE********************

#pragma critical
{
            REF.p_READY_FOR_INTEGRATION->push(ix);
}
            ++total_processed;

        } else {
            // *************THIS IS WHAT HAPPENS IF THERE WAS NO INDEX UPDATE****************
            tot_locals  = 0;
            sent_locally = 0;
            total_processed = 0;
            //answer_messages<DT, TIMETOL, BATCH>(REF, O.MY_THREAD_n); // #HYPERPARAMS #HYPERPARAMETERS
        }
        ix_update = false;
#pragma omp critical
{
        if (!REF.p_CHECKED->empty()) {
            ix = REF.p_CHECKED->front();
            REF.p_CHECKED->pop();
            ix_update = true;
        }
}
        // Add to the number of totals processed the ones from previous lap :-)
        if (total_processed != 0) {
#pragma omp atomic update
            *(REF.p_TOT) += total_processed;
        }

        // if there was no available index, check if it is still worth looping.
        if (!ix_update) {
#pragma omp atomic read
            atomic_helper = *(REF.p_TOT);
            globalstatus = (atomic_helper < NNodes);
        }
        PRINTF_DBG("TOT=%d, NNodes=%d, ix_update=%d, total_processed=%d, ix=%d\n",
               atomic_helper, NNodes, ix_update, total_processed, ix);std::cout<<std::flush;
    }
    PRINTF_DBG("Final termination of perform_requests :-)\n");std::cout<<std::flush;

} // end of function

